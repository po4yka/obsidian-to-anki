"""Pydantic models for multi-agent system validation and results."""

from typing import Literal

from pydantic import BaseModel, ConfigDict, Field


class NoteMetadata(BaseModel):
    """Lightweight metadata model used by agent components."""

    model_config = ConfigDict(extra="allow")

    id: str | None = Field(default=None, description="Optional note identifier")
    title: str = Field(default="", description="Note title")
    topic: str = Field(default="", description="Primary note topic")
    tags: list[str] = Field(default_factory=list, description="Associated tags")
    file_path: str | None = Field(default=None, description="Source file path")
    language_tags: list[str] = Field(
        default_factory=list, description="Language tags for the note"
    )


class QualityDimension(BaseModel):
    """Quality assessment for a specific dimension."""

    model_config = ConfigDict(frozen=False)

    score: float = Field(
        default=0.5, ge=0.0, le=1.0, description="Dimension quality score"
    )
    weight: float = Field(
        default=0.5, ge=0.0, le=1.0, description="Dimension weight in overall score"
    )
    issues: list[str] = Field(default_factory=list, description="Specific issues found")
    strengths: list[str] = Field(
        default_factory=list, description="Positive aspects identified"
    )


class QualityReport(BaseModel):
    """Comprehensive quality assessment report for a card."""

    model_config = ConfigDict(frozen=False)

    overall_score: float = Field(
        default=0.5, ge=0.0, le=1.0, description="Overall quality score"
    )
    dimensions: dict[str, QualityDimension] = Field(
        default_factory=dict, description="Quality scores by dimension"
    )
    suggestions: list[str] = Field(
        default_factory=list, description="Actionable improvement suggestions"
    )
    confidence: float = Field(
        default=0.5, ge=0.0, le=1.0, description="Confidence in assessment"
    )
    assessment_time: float = Field(
        default=0.0, ge=0.0, description="Time taken for assessment"
    )


class PreValidationResult(BaseModel):
    """Result from pre-validator agent.

    The pre-validator checks note structure, formatting, and frontmatter
    before expensive card generation.
    """

    model_config = ConfigDict(frozen=False)

    is_valid: bool
    error_type: Literal["format", "structure", "frontmatter", "content", "none"]
    error_details: str = ""
    auto_fix_applied: bool = False
    fixed_content: str | None = None
    validation_time: float = 0.0


class GeneratedCard(BaseModel):
    """Single card generated by generator agent.

    This represents an APF card with metadata for tracking.
    """

    model_config = ConfigDict(frozen=False)

    card_index: int = Field(ge=1, description="1-based card index")
    slug: str = Field(min_length=1, description="Unique card identifier")
    lang: str = Field(pattern="^(en|ru)$", description="Card language")
    apf_html: str = Field(min_length=1, description="APF HTML content")
    confidence: float = Field(ge=0.0, le=1.0, description="Generation confidence score")
    content_hash: str = Field(
        default="", description="Stable content hash for change detection"
    )


class GenerationResult(BaseModel):
    """Result from generator agent.

    Contains all cards generated from a single note.
    """

    model_config = ConfigDict(frozen=False)

    cards: list[GeneratedCard]
    total_cards: int = Field(ge=0)
    generation_time: float = Field(ge=0.0)
    model_used: str


class PostValidationResult(BaseModel):
    """Result from post-validator agent.

    Validates generated cards for syntax, factual accuracy, and coherence.
    """

    model_config = ConfigDict(frozen=False)

    is_valid: bool
    error_type: Literal["syntax", "factual", "semantic", "template", "none"]
    error_details: str = ""
    corrected_cards: list[GeneratedCard] | None = None
    validation_time: float = 0.0


class MemorizationQualityResult(BaseModel):
    """Result from memorization quality agent.

    Evaluates whether cards are effective for spaced repetition learning.
    """

    model_config = ConfigDict(frozen=False)

    is_memorizable: bool
    memorization_score: float = Field(default=0.5, ge=0.0, le=1.0)
    issues: list[dict[str, str]] = Field(default_factory=list)
    strengths: list[str] = Field(default_factory=list)
    suggested_improvements: list[str] = Field(default_factory=list)
    assessment_time: float = 0.0


class CardSplitPlan(BaseModel):
    """Plan for a single card in a split."""

    model_config = ConfigDict(frozen=False)

    card_number: int = Field(default=1, ge=1)
    concept: str = Field(min_length=1)
    question: str = Field(min_length=1)
    answer_summary: str = Field(min_length=1)
    rationale: str = Field(default="")


class CardSplittingResult(BaseModel):
    """Result from card splitting agent.

    Determines if note should generate one or multiple cards.
    """

    model_config = ConfigDict(frozen=False)

    should_split: bool
    card_count: int = Field(default=1, ge=1)
    splitting_strategy: Literal[
        "none",
        "concept",
        "list",
        "example",
        "hierarchical",
        "step",
        "difficulty",
        "prerequisite",
        "context_aware",
    ]
    split_plan: list[CardSplitPlan] = Field(default_factory=list)
    reasoning: str = ""
    decision_time: float = 0.0
    confidence: float = Field(
        ge=0.0, le=1.0, default=0.5, description="Confidence in splitting decision"
    )
    fallback_strategy: str | None = Field(
        default=None, description="Fallback strategy if primary fails"
    )


class RepairDiagnosis(BaseModel):
    """Structured error diagnosis for parser repair.

    Categorizes errors with severity and priority for repair.
    """

    model_config = ConfigDict(frozen=False)

    error_category: Literal[
        "syntax", "structure", "content", "quality", "frontmatter", "unknown"
    ] = Field(description="Category of the error")
    severity: Literal["low", "medium", "high", "critical"] = Field(
        description="Severity of the error"
    )
    error_description: str = Field(description="Detailed description of the error")
    repair_priority: int = Field(
        default=5, ge=1, le=10, description="Repair priority (1=highest, 10=lowest)"
    )
    can_auto_fix: bool = Field(description="Whether this error can be auto-fixed")


class RepairQualityScore(BaseModel):
    """Quality scoring for note content before and after repair."""

    model_config = ConfigDict(frozen=False)

    completeness_score: float = Field(
        default=0.5, ge=0.0, le=1.0, description="Content completeness (0-1)"
    )
    structure_score: float = Field(
        default=0.5, ge=0.0, le=1.0, description="Structure quality (0-1)"
    )
    bilingual_consistency: float = Field(
        default=0.5, ge=0.0, le=1.0, description="Consistency between languages (0-1)"
    )
    technical_accuracy: float = Field(
        default=0.5, ge=0.0, le=1.0, description="Technical accuracy score (0-1)"
    )
    overall_score: float = Field(
        default=0.5, ge=0.0, le=1.0, description="Overall quality score (0-1)"
    )
    issues_found: list[str] = Field(
        default_factory=list, description="List of quality issues identified"
    )


class ParserRepairResult(BaseModel):
    """Result from parser-repair agent.

    Tracks diagnosis and repairs applied to malformed notes.
    """

    model_config = ConfigDict(frozen=False)

    is_repairable: bool
    diagnosis: str = ""
    repairs: list[dict[str, str]] = Field(default_factory=list)
    repaired_content: str | None = None
    repair_time: float = 0.0
    error_diagnosis: RepairDiagnosis | None = None
    quality_before: RepairQualityScore | None = None
    quality_after: RepairQualityScore | None = None


class DuplicateMatch(BaseModel):
    """A potential duplicate card match."""

    model_config = ConfigDict(frozen=False)

    card_slug: str = Field(min_length=1, description="Slug of potential duplicate card")
    similarity_score: float = Field(
        default=0.0, ge=0.0, le=1.0, description="Similarity score"
    )
    duplicate_type: Literal["exact", "semantic", "partial_overlap", "unique"]
    reasoning: str = Field(default="", description="Why this is considered a duplicate")


class DuplicateDetectionResult(BaseModel):
    """Result from duplicate detection agent.

    Identifies redundant or overlapping cards.
    """

    model_config = ConfigDict(frozen=False)

    is_duplicate: bool
    best_match: DuplicateMatch | None = None
    all_matches: list[DuplicateMatch] = Field(default_factory=list)
    recommendation: Literal["delete", "merge", "keep_both", "review_manually"]
    better_card: str | None = Field(
        default=None,
        description="Slug of better card if duplicate ('new' or existing slug)",
    )
    merge_suggestion: str | None = None
    detection_time: float = 0.0


class EnrichmentAddition(BaseModel):
    """A specific enrichment added to a card."""

    model_config = ConfigDict(frozen=False)

    enrichment_type: Literal["example", "mnemonic", "visual", "related", "practical"]
    content: str = Field(min_length=1)
    rationale: str = Field(default="")


class ContextEnrichmentResult(BaseModel):
    """Result from context enrichment agent.

    Enhances cards with examples, mnemonics, and context.
    """

    model_config = ConfigDict(frozen=False)

    should_enrich: bool
    enriched_card: GeneratedCard | None = None
    additions: list[EnrichmentAddition] = Field(default_factory=list)
    additions_summary: str = ""
    enrichment_rationale: str = ""
    enrichment_time: float = 0.0


class NoteCorrectionResult(BaseModel):
    """Result from proactive note correction agent.

    Tracks quality analysis and corrections applied before parsing.
    """

    model_config = ConfigDict(frozen=False)

    needs_correction: bool = Field(description="Whether the note needs correction")
    corrected_content: str | None = Field(
        default=None, description="Corrected note content if repairs were applied"
    )
    quality_score: float = Field(
        default=0.5,
        ge=0.0,
        le=1.0,
        description="Overall quality score before correction",
    )
    issues_found: list[str] = Field(
        default_factory=list, description="Issues detected in the note"
    )
    corrections_applied: list[str] = Field(
        default_factory=list, description="List of corrections applied"
    )
    confidence: float = Field(
        default=0.5, ge=0.0, le=1.0, description="Confidence in correction quality"
    )
    correction_time: float = Field(
        default=0.0, ge=0.0, description="Time taken for correction"
    )
    quality_after: RepairQualityScore | None = Field(
        default=None, description="Quality score after correction"
    )


class RepairStrategy(BaseModel):
    """Repair strategy selection based on error type."""

    model_config = ConfigDict(frozen=False)

    strategy_type: Literal[
        "deterministic",
        "rule_based",
        "llm_based",
        "multi_stage",
        "partial",
        "skip",
    ] = Field(description="Type of repair strategy to use")
    priority: int = Field(
        default=5, ge=1, le=10, description="Repair priority (1=highest, 10=lowest)"
    )
    stages: list[str] = Field(
        default_factory=list,
        description="Ordered list of repair stages (e.g., ['syntax', 'structure', 'content'])",
    )
    confidence_threshold: float = Field(
        default=0.7,
        ge=0.0,
        le=1.0,
        description="Minimum confidence required to apply this strategy",
    )


class PartialRepairResult(BaseModel):
    """Result from partial repair operation.

    Supports fixing what can be fixed while flagging what cannot.
    """

    model_config = ConfigDict(frozen=False)

    repaired_content: str = Field(description="Partially or fully repaired content")
    sections_fixed: list[str] = Field(
        default_factory=list,
        description="List of sections that were successfully fixed",
    )
    sections_failed: list[str] = Field(
        default_factory=list, description="List of sections that could not be fixed"
    )
    section_confidence: dict[str, float] = Field(
        default_factory=dict,
        description="Confidence score per section (0.0-1.0)",
    )
    is_complete: bool = Field(
        description="Whether all sections were successfully repaired"
    )
    repair_report: str = Field(
        default="", description="Detailed report of repair status per section"
    )


class AgentPipelineResult(BaseModel):
    """Complete result from the multi-agent pipeline.

    Tracks results from pre-validation, generation, post-validation, and memorization quality stages.
    """

    model_config = ConfigDict(frozen=False)

    success: bool
    pre_validation: PreValidationResult
    generation: GenerationResult | None = None
    post_validation: PostValidationResult | None = None
    memorization_quality: MemorizationQualityResult | None = None
    note_correction: NoteCorrectionResult | None = Field(
        default=None, description="Proactive note correction result"
    )
    total_time: float = Field(ge=0.0)
    retry_count: int = Field(ge=0, description="Number of post-validation retries")


class SplitValidationResult(BaseModel):
    """Result from split validator agent.

    Validates whether the proposed card split is necessary and optimal.
    """

    model_config = ConfigDict(frozen=False)

    is_valid: bool
    validation_score: float = Field(default=0.5, ge=0.0, le=1.0)
    feedback: str = ""
    suggested_modifications: list[str] = Field(default_factory=list)
    validation_time: float = 0.0
