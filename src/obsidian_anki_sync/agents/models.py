"""Pydantic models for multi-agent system validation and results."""

from __future__ import annotations

from enum import Enum
from typing import Literal

from pydantic import BaseModel, ConfigDict, Field, field_validator


class CardCorrection(BaseModel):
    """Suggested correction for a card field (patch model).

    This represents a field-level patch, not a full card replacement.
    Use with apply_corrections() to apply patches to GeneratedCard objects.
    """

    model_config = ConfigDict(frozen=False)

    card_index: int = Field(ge=0, description="0-based card index")
    field_name: str = Field(min_length=1, description="Name of the field to correct")
    current_value: str | None = Field(default=None, description="Current field value")
    suggested_value: str = Field(description="Suggested corrected value")
    rationale: str = Field(default="", description="Reason for the correction")


class NoteMetadata(BaseModel):
    """Lightweight metadata model used by agent components."""

    model_config = ConfigDict(extra="allow")

    id: str | None = Field(
        default=None, description="Optional note identifier")
    title: str = Field(default="", description="Note title")
    topic: str = Field(default="", description="Primary note topic")
    tags: list[str] = Field(default_factory=list,
                            description="Associated tags")
    file_path: str | None = Field(default=None, description="Source file path")
    language_tags: list[str] = Field(
        default_factory=list, description="Language tags for the note"
    )


class QualityDimension(BaseModel):
    """Quality assessment for a specific dimension."""

    model_config = ConfigDict(frozen=False)

    score: float = Field(
        default=0.5, ge=0.0, le=1.0, description="Dimension quality score"
    )
    weight: float = Field(
        default=0.5, ge=0.0, le=1.0, description="Dimension weight in overall score"
    )
    issues: list[str] = Field(default_factory=list,
                              description="Specific issues found")
    strengths: list[str] = Field(
        default_factory=list, description="Positive aspects identified"
    )


class QualityReport(BaseModel):
    """Comprehensive quality assessment report for a card."""

    model_config = ConfigDict(frozen=False)

    overall_score: float = Field(
        default=0.5, ge=0.0, le=1.0, description="Overall quality score"
    )
    dimensions: dict[str, QualityDimension] = Field(
        default_factory=dict, description="Quality scores by dimension"
    )
    suggestions: list[str] = Field(
        default_factory=list, description="Actionable improvement suggestions"
    )
    confidence: float = Field(
        default=0.5, ge=0.0, le=1.0, description="Confidence in assessment"
    )
    assessment_time: float = Field(
        default=0.0, ge=0.0, description="Time taken for assessment"
    )


class PreValidationResult(BaseModel):
    """Result from pre-validator agent.

    The pre-validator checks note structure, formatting, and frontmatter
    before expensive card generation.
    """

    model_config = ConfigDict(frozen=False)

    is_valid: bool
    error_type: Literal["format", "structure",
                        "frontmatter", "content", "none"]
    error_details: str = ""
    auto_fix_applied: bool = False
    fixed_content: str | None = None
    validation_time: float = 0.0


class GeneratedCard(BaseModel):
    """Single card generated by generator agent.

    This represents an APF card with metadata for tracking.
    """

    model_config = ConfigDict(frozen=False)

    card_index: int = Field(ge=1, description="1-based card index")
    slug: str = Field(min_length=1, description="Unique card identifier")
    lang: str = Field(pattern="^(en|ru)$", description="Card language")
    apf_html: str = Field(min_length=1, description="APF HTML content")
    confidence: float = Field(
        ge=0.0, le=1.0, description="Generation confidence score")
    content_hash: str = Field(
        default="", description="Stable content hash for change detection"
    )


class GenerationResult(BaseModel):
    """Result from generator agent.

    Contains all cards generated from a single note.
    """

    model_config = ConfigDict(frozen=False)

    cards: list[GeneratedCard]
    total_cards: int = Field(ge=0)
    generation_time: float = Field(ge=0.0)
    model_used: str


class PostValidationResult(BaseModel):
    """Result from post-validator agent.

    Validates generated cards for syntax, factual accuracy, and coherence.
    Supports field-level patches via suggested_corrections.
    """

    model_config = ConfigDict(frozen=False)

    is_valid: bool
    error_type: Literal["syntax", "factual", "semantic", "template", "none"]
    error_details: str = ""
    suggested_corrections: list[CardCorrection] = Field(
        default_factory=list,
        description="Field-level corrections suggested by validator",
    )
    corrected_cards: list[GeneratedCard] | None = Field(
        default=None,
        description="Cards after applying suggested_corrections patches",
    )
    applied_changes: list[str] = Field(
        default_factory=list,
        description="Human-readable descriptions of applied corrections",
    )
    validation_time: float = 0.0
    structured_errors: list[dict] | None = Field(
        default=None, description="Structured validation errors"
    )


class MemorizationQualityResult(BaseModel):
    """Result from memorization quality agent.

    Evaluates whether cards are effective for spaced repetition learning.
    """

    model_config = ConfigDict(frozen=False)

    is_memorizable: bool
    memorization_score: float = Field(default=0.5, ge=0.0, le=1.0)
    issues: list[dict[str, str]] = Field(default_factory=list)
    strengths: list[str] = Field(default_factory=list)
    suggested_improvements: list[str] = Field(default_factory=list)
    assessment_time: float = 0.0


class CardSplitPlan(BaseModel):
    """Plan for a single card in a split."""

    model_config = ConfigDict(frozen=False)

    card_number: int = Field(default=1, ge=1)
    concept: str = Field(min_length=1)
    question: str = Field(min_length=1)
    answer_summary: str = Field(min_length=1)
    rationale: str = Field(default="")


class CardSplittingResult(BaseModel):
    """Result from card splitting agent.

    Determines if note should generate one or multiple cards.
    """

    model_config = ConfigDict(frozen=False)

    should_split: bool
    card_count: int = Field(default=1, ge=1)
    splitting_strategy: Literal[
        "none",
        "concept",
        "list",
        "example",
        "hierarchical",
        "step",
        "difficulty",
        "prerequisite",
        "context_aware",
    ]
    split_plan: list[CardSplitPlan] = Field(default_factory=list)
    reasoning: str = ""
    decision_time: float = 0.0
    confidence: float = Field(
        ge=0.0, le=1.0, default=0.5, description="Confidence in splitting decision"
    )
    fallback_strategy: str | None = Field(
        default=None, description="Fallback strategy if primary fails"
    )


class RepairDiagnosis(BaseModel):
    """Structured error diagnosis for parser repair.

    Categorizes errors with severity and priority for repair.
    """

    model_config = ConfigDict(frozen=False)

    error_category: Literal[
        "syntax", "structure", "content", "quality", "frontmatter", "unknown"
    ] = Field(description="Category of the error")
    severity: Literal["low", "medium", "high", "critical"] = Field(
        description="Severity of the error"
    )
    error_description: str = Field(
        description="Detailed description of the error")
    repair_priority: int = Field(
        default=5, ge=1, le=10, description="Repair priority (1=highest, 10=lowest)"
    )
    can_auto_fix: bool = Field(
        description="Whether this error can be auto-fixed")


class RepairQualityScore(BaseModel):
    """Quality scoring for note content before and after repair."""

    model_config = ConfigDict(frozen=False)

    completeness_score: float = Field(
        default=0.5, ge=0.0, le=1.0, description="Content completeness (0-1)"
    )
    structure_score: float = Field(
        default=0.5, ge=0.0, le=1.0, description="Structure quality (0-1)"
    )
    bilingual_consistency: float = Field(
        default=0.5, ge=0.0, le=1.0, description="Consistency between languages (0-1)"
    )
    technical_accuracy: float = Field(
        default=0.5, ge=0.0, le=1.0, description="Technical accuracy score (0-1)"
    )
    overall_score: float = Field(
        default=0.5, ge=0.0, le=1.0, description="Overall quality score (0-1)"
    )
    issues_found: list[str] = Field(
        default_factory=list, description="List of quality issues identified"
    )


class ParserRepairResult(BaseModel):
    """Result from parser-repair agent.

    Tracks diagnosis and repairs applied to malformed notes.
    """

    model_config = ConfigDict(frozen=False)

    is_repairable: bool
    diagnosis: str = ""
    repairs: list[dict[str, str]] = Field(default_factory=list)
    repaired_content: str | None = None
    repair_time: float = 0.0
    error_diagnosis: RepairDiagnosis | None = None
    quality_before: RepairQualityScore | None = None
    quality_after: RepairQualityScore | None = None


class DuplicateMatch(BaseModel):
    """A potential duplicate card match."""

    model_config = ConfigDict(frozen=False)

    card_slug: str = Field(
        min_length=1, description="Slug of potential duplicate card")
    similarity_score: float = Field(
        default=0.0, ge=0.0, le=1.0, description="Similarity score"
    )
    duplicate_type: Literal["exact", "semantic", "partial_overlap", "unique"]
    reasoning: str = Field(
        default="", description="Why this is considered a duplicate")


class DuplicateDetectionResult(BaseModel):
    """Result from duplicate detection agent.

    Identifies redundant or overlapping cards.
    """

    model_config = ConfigDict(frozen=False)

    is_duplicate: bool
    best_match: DuplicateMatch | None = None
    all_matches: list[DuplicateMatch] = Field(default_factory=list)
    recommendation: Literal["delete", "merge", "keep_both", "review_manually"]
    better_card: str | None = Field(
        default=None,
        description="Slug of better card if duplicate ('new' or existing slug)",
    )
    merge_suggestion: str | None = None
    detection_time: float = 0.0


class EnrichmentType(str, Enum):
    """Normalized enrichment type aligned with APF Doc B taxonomy."""

    EXAMPLE = "example"
    MNEMONIC = "mnemonic"
    VISUAL = "visual"
    RELATED = "related"
    PRACTICAL = "practical"

    @classmethod
    def from_value(
        cls, value: EnrichmentType | str, *, apply_aliases: bool = True
    ) -> EnrichmentType:
        """Normalize arbitrary labels into a supported enrichment type."""
        if isinstance(value, cls):
            return value

        normalized = value.strip().lower()
        normalized = normalized.replace("-", "_").replace(" ", "_")

        if apply_aliases:
            normalized = ENRICHMENT_TYPE_ALIASES.get(normalized, normalized)

        return cls(normalized)


ENRICHMENT_TYPE_ALIASES: dict[str, str] = {
    # Doc B discourages loose math/science tags; map to related context
    "math_science": EnrichmentType.RELATED.value,
    "mathscience": EnrichmentType.RELATED.value,
    "math_context": EnrichmentType.RELATED.value,
    "science_context": EnrichmentType.RELATED.value,
    "formula_context": EnrichmentType.RELATED.value,
    # Friendly synonyms used by some LLMs
    "analogy": EnrichmentType.EXAMPLE.value,
    "story": EnrichmentType.MNEMONIC.value,
    "diagram": EnrichmentType.VISUAL.value,
    "comparison": EnrichmentType.VISUAL.value,
    "workflow": EnrichmentType.PRACTICAL.value,
}


def normalize_enrichment_label(
    value: str, *, apply_aliases: bool = True
) -> str:
    """Return canonical enrichment label, optionally skipping alias mapping."""
    return EnrichmentType.from_value(value, apply_aliases=apply_aliases).value


class EnrichmentAddition(BaseModel):
    """A specific enrichment added to a card."""

    model_config = ConfigDict(frozen=False)

    enrichment_type: EnrichmentType
    content: str = Field(min_length=1)
    rationale: str = Field(default="")

    @field_validator("enrichment_type", mode="before")
    @classmethod
    def _normalize_enrichment_type(
        cls, value: EnrichmentType | str
    ) -> EnrichmentType:
        """Coerce arbitrary labels into canonical enrichment types."""
        return EnrichmentType.from_value(value)


class ContextEnrichmentResult(BaseModel):
    """Result from context enrichment agent.

    Enhances cards with examples, mnemonics, and context.
    """

    model_config = ConfigDict(frozen=False)

    should_enrich: bool
    enriched_card: GeneratedCard | None = None
    additions: list[EnrichmentAddition] = Field(default_factory=list)
    additions_summary: str = ""
    enrichment_rationale: str = ""
    enrichment_time: float = 0.0


class HighlightedQA(BaseModel):
    """A candidate question/answer pair extracted from a note."""

    model_config = ConfigDict(frozen=False)

    question: str = Field(min_length=1, description="Candidate question text")
    answer: str = Field(min_length=1, description="Candidate answer text")
    confidence: float = Field(
        default=0.0, ge=0.0, le=1.0, description="Confidence in this highlight"
    )
    source_excerpt: str | None = Field(
        default=None, description="Excerpt or anchor describing where the QA was found"
    )
    anchor: str | None = Field(
        default=None, description="Optional anchor or heading reference inside the note"
    )


class HighlightResult(BaseModel):
    """Result from highlight agent summarizing potential QA pairs."""

    model_config = ConfigDict(frozen=False)

    qa_candidates: list[HighlightedQA] = Field(
        default_factory=list, description="List of highlighted QA candidates"
    )
    summaries: list[str] = Field(
        default_factory=list, description="High-level summaries of note content"
    )
    suggestions: list[str] = Field(
        default_factory=list,
        description="Actionable suggestions for improving the note",
    )
    detected_sections: list[str] = Field(
        default_factory=list, description="Sections detected during analysis"
    )
    confidence: float = Field(
        default=0.0,
        ge=0.0,
        le=1.0,
        description="Confidence that the highlights are helpful",
    )
    note_status: Literal["draft", "incomplete", "ready", "unknown"] = Field(
        default="unknown", description="Inferred note status based on analysis"
    )
    analysis_time: float = Field(
        default=0.0, ge=0.0, description="Time spent running the highlight agent"
    )
    raw_excerpt: str | None = Field(
        default=None,
        description="Optional raw excerpt containing the highlighted content",
    )


class NoteCorrectionResult(BaseModel):
    """Result from proactive note correction agent.

    Tracks quality analysis and corrections applied before parsing.
    """

    model_config = ConfigDict(frozen=False)

    needs_correction: bool = Field(
        description="Whether the note needs correction")
    corrected_content: str | None = Field(
        default=None, description="Corrected note content if repairs were applied"
    )
    quality_score: float = Field(
        default=0.5,
        ge=0.0,
        le=1.0,
        description="Overall quality score before correction",
    )
    issues_found: list[str] = Field(
        default_factory=list, description="Issues detected in the note"
    )
    corrections_applied: list[str] = Field(
        default_factory=list, description="List of corrections applied"
    )
    confidence: float = Field(
        default=0.5, ge=0.0, le=1.0, description="Confidence in correction quality"
    )
    correction_time: float = Field(
        default=0.0, ge=0.0, description="Time taken for correction"
    )
    quality_after: RepairQualityScore | None = Field(
        default=None, description="Quality score after correction"
    )


class RepairStrategy(BaseModel):
    """Repair strategy selection based on error type."""

    model_config = ConfigDict(frozen=False)

    strategy_type: Literal[
        "deterministic",
        "rule_based",
        "llm_based",
        "multi_stage",
        "partial",
        "skip",
    ] = Field(description="Type of repair strategy to use")
    priority: int = Field(
        default=5, ge=1, le=10, description="Repair priority (1=highest, 10=lowest)"
    )
    stages: list[str] = Field(
        default_factory=list,
        description="Ordered list of repair stages (e.g., ['syntax', 'structure', 'content'])",
    )
    confidence_threshold: float = Field(
        default=0.7,
        ge=0.0,
        le=1.0,
        description="Minimum confidence required to apply this strategy",
    )


class PartialRepairResult(BaseModel):
    """Result from partial repair operation.

    Supports fixing what can be fixed while flagging what cannot.
    """

    model_config = ConfigDict(frozen=False)

    repaired_content: str = Field(
        description="Partially or fully repaired content")
    sections_fixed: list[str] = Field(
        default_factory=list,
        description="List of sections that were successfully fixed",
    )
    sections_failed: list[str] = Field(
        default_factory=list, description="List of sections that could not be fixed"
    )
    section_confidence: dict[str, float] = Field(
        default_factory=dict,
        description="Confidence score per section (0.0-1.0)",
    )
    is_complete: bool = Field(
        description="Whether all sections were successfully repaired"
    )
    repair_report: str = Field(
        default="", description="Detailed report of repair status per section"
    )


class AgentPipelineResult(BaseModel):
    """Complete result from the multi-agent pipeline.

    Tracks results from pre-validation, generation, post-validation, and memorization quality stages.
    """

    model_config = ConfigDict(frozen=False)

    success: bool
    pre_validation: PreValidationResult
    generation: GenerationResult | None = None
    post_validation: PostValidationResult | None = None
    memorization_quality: MemorizationQualityResult | None = None
    highlight_result: HighlightResult | None = Field(
        default=None,
        description="Highlight analysis when Q/A content is missing or weak",
    )
    note_correction: NoteCorrectionResult | None = Field(
        default=None, description="Proactive note correction result"
    )
    total_time: float = Field(ge=0.0)
    retry_count: int = Field(
        ge=0, description="Number of post-validation retries")
    stage_times: dict[str, float] = Field(
        default_factory=dict,
        description="Per-stage execution times (seconds)",
    )
    last_error: str | None = Field(
        default=None, description="Last error recorded by the pipeline"
    )


class SplitValidationResult(BaseModel):
    """Result from split validator agent.

    Validates whether the proposed card split is necessary and optimal.
    """

    model_config = ConfigDict(frozen=False)

    is_valid: bool
    validation_score: float = Field(default=0.5, ge=0.0, le=1.0)
    feedback: str = ""
    suggested_modifications: list[str] = Field(default_factory=list)
    validation_time: float = 0.0


class AutoFixIssue(BaseModel):
    """A single auto-fixable issue found in a note."""

    model_config = ConfigDict(frozen=False)

    issue_type: Literal[
        "trailing_whitespace",
        "empty_references",
        "title_format",
        "moc_mismatch",
        "section_order",
        "missing_related_questions",
        "broken_wikilink",
        "broken_related_entry",
        "missing_concept_links",
        "related_count",
        "unknown_error",
    ] = Field(description="Type of issue detected")
    severity: Literal["info", "warning", "error"] = Field(
        default="warning", description="Issue severity"
    )
    description: str = Field(
        default="", description="Human-readable issue description")
    location: str | None = Field(
        default=None, description="Location in file (line number or field name)"
    )
    auto_fixed: bool = Field(
        default=False, description="Whether this issue was fixed")
    fix_description: str = Field(
        default="", description="Description of the fix applied"
    )


class AutoFixResult(BaseModel):
    """Result from auto-fix agent during note scanning.

    Tracks issues detected and fixes applied to notes before card generation.
    """

    model_config = ConfigDict(frozen=False)

    file_modified: bool = Field(
        default=False, description="Whether the source file was modified"
    )
    issues_found: list[AutoFixIssue] = Field(
        default_factory=list, description="All issues detected"
    )
    issues_fixed: int = Field(
        default=0, description="Number of issues auto-fixed")
    issues_skipped: int = Field(
        default=0, description="Number of issues that could not be fixed"
    )
    original_content: str | None = Field(
        default=None, description="Original content before fixes (for rollback)"
    )
    fixed_content: str | None = Field(
        default=None, description="Content after fixes applied"
    )
    fix_time: float = Field(default=0.0, ge=0.0,
                            description="Time taken for fixes")
    write_back_enabled: bool = Field(
        default=False, description="Whether fixes were written back to file"
    )
