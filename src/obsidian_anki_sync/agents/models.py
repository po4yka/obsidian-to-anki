"""Pydantic models for multi-agent system validation and results."""

from typing import Literal, Optional

from pydantic import BaseModel, ConfigDict, Field


class PreValidationResult(BaseModel):
    """Result from pre-validator agent.

    The pre-validator checks note structure, formatting, and frontmatter
    before expensive card generation.
    """

    model_config = ConfigDict(frozen=False)

    is_valid: bool
    error_type: Literal["format", "structure",
                        "frontmatter", "content", "none"]
    error_details: str = ""
    auto_fix_applied: bool = False
    fixed_content: Optional[str] = None
    validation_time: float = 0.0


class GeneratedCard(BaseModel):
    """Single card generated by generator agent.

    This represents an APF card with metadata for tracking.
    """

    model_config = ConfigDict(frozen=False)

    card_index: int = Field(ge=1, description="1-based card index")
    slug: str = Field(min_length=1, description="Unique card identifier")
    lang: str = Field(pattern="^(en|ru)$", description="Card language")
    apf_html: str = Field(min_length=1, description="APF HTML content")
    confidence: float = Field(
        ge=0.0, le=1.0, description="Generation confidence score")
    content_hash: str = Field(
        default="", description="Stable content hash for change detection"
    )


class GenerationResult(BaseModel):
    """Result from generator agent.

    Contains all cards generated from a single note.
    """

    model_config = ConfigDict(frozen=False)

    cards: list[GeneratedCard]
    total_cards: int = Field(ge=0)
    generation_time: float = Field(ge=0.0)
    model_used: str


class PostValidationResult(BaseModel):
    """Result from post-validator agent.

    Validates generated cards for syntax, factual accuracy, and coherence.
    """

    model_config = ConfigDict(frozen=False)

    is_valid: bool
    error_type: Literal["syntax", "factual", "semantic", "template", "none"]
    error_details: str = ""
    corrected_cards: Optional[list[GeneratedCard]] = None
    validation_time: float = 0.0


class MemorizationQualityResult(BaseModel):
    """Result from memorization quality agent.

    Evaluates whether cards are effective for spaced repetition learning.
    """

    model_config = ConfigDict(frozen=False)

    is_memorizable: bool
    memorization_score: float = Field(ge=0.0, le=1.0)
    issues: list[dict[str, str]] = Field(default_factory=list)
    strengths: list[str] = Field(default_factory=list)
    suggested_improvements: list[str] = Field(default_factory=list)
    assessment_time: float = 0.0


class CardSplitPlan(BaseModel):
    """Plan for a single card in a split."""

    model_config = ConfigDict(frozen=False)

    card_number: int = Field(ge=1)
    concept: str = Field(min_length=1)
    question: str = Field(min_length=1)
    answer_summary: str = Field(min_length=1)
    rationale: str = Field(default="")


class CardSplittingResult(BaseModel):
    """Result from card splitting agent.

    Determines if note should generate one or multiple cards.
    """

    model_config = ConfigDict(frozen=False)

    should_split: bool
    card_count: int = Field(ge=1)
    splitting_strategy: Literal[
        "none",
        "concept",
        "list",
        "example",
        "hierarchical",
        "step",
        "difficulty",
        "prerequisite",
        "context_aware",
    ]
    split_plan: list[CardSplitPlan] = Field(default_factory=list)
    reasoning: str = ""
    decision_time: float = 0.0
    confidence: float = Field(
        ge=0.0, le=1.0, default=0.5, description="Confidence in splitting decision"
    )
    fallback_strategy: Optional[str] = Field(
        default=None, description="Fallback strategy if primary fails"
    )


class RepairDiagnosis(BaseModel):
    """Structured error diagnosis for parser repair.

    Categorizes errors with severity and priority for repair.
    """

    model_config = ConfigDict(frozen=False)

    error_category: Literal[
        "syntax", "structure", "content", "quality", "frontmatter", "unknown"
    ] = Field(description="Category of the error")
    severity: Literal["low", "medium", "high", "critical"] = Field(
        description="Severity of the error"
    )
    error_description: str = Field(
        description="Detailed description of the error")
    repair_priority: int = Field(
        ge=1, le=10, description="Repair priority (1=highest, 10=lowest)"
    )
    can_auto_fix: bool = Field(
        description="Whether this error can be auto-fixed")


class RepairQualityScore(BaseModel):
    """Quality scoring for note content before and after repair."""

    model_config = ConfigDict(frozen=False)

    completeness_score: float = Field(
        ge=0.0, le=1.0, description="Content completeness (0-1)"
    )
    structure_score: float = Field(
        ge=0.0, le=1.0, description="Structure quality (0-1)"
    )
    bilingual_consistency: float = Field(
        ge=0.0, le=1.0, description="Consistency between languages (0-1)"
    )
    technical_accuracy: float = Field(
        ge=0.0, le=1.0, description="Technical accuracy score (0-1)"
    )
    overall_score: float = Field(
        ge=0.0, le=1.0, description="Overall quality score (0-1)"
    )
    issues_found: list[str] = Field(
        default_factory=list, description="List of quality issues identified"
    )


class ParserRepairResult(BaseModel):
    """Result from parser-repair agent.

    Tracks diagnosis and repairs applied to malformed notes.
    """

    model_config = ConfigDict(frozen=False)

    is_repairable: bool
    diagnosis: str = ""
    repairs: list[dict[str, str]] = Field(default_factory=list)
    repaired_content: Optional[str] = None
    repair_time: float = 0.0
    error_diagnosis: Optional[RepairDiagnosis] = None
    quality_before: Optional[RepairQualityScore] = None
    quality_after: Optional[RepairQualityScore] = None


class DuplicateMatch(BaseModel):
    """A potential duplicate card match."""

    model_config = ConfigDict(frozen=False)

    card_slug: str = Field(
        min_length=1, description="Slug of potential duplicate card")
    similarity_score: float = Field(
        ge=0.0, le=1.0, description="Similarity score")
    duplicate_type: Literal["exact", "semantic", "partial_overlap", "unique"]
    reasoning: str = Field(
        default="", description="Why this is considered a duplicate")


class DuplicateDetectionResult(BaseModel):
    """Result from duplicate detection agent.

    Identifies redundant or overlapping cards.
    """

    model_config = ConfigDict(frozen=False)

    is_duplicate: bool
    best_match: Optional[DuplicateMatch] = None
    all_matches: list[DuplicateMatch] = Field(default_factory=list)
    recommendation: Literal["delete", "merge", "keep_both", "review_manually"]
    better_card: Optional[str] = Field(
        default=None,
        description="Slug of better card if duplicate ('new' or existing slug)",
    )
    merge_suggestion: Optional[str] = None
    detection_time: float = 0.0


class EnrichmentAddition(BaseModel):
    """A specific enrichment added to a card."""

    model_config = ConfigDict(frozen=False)

    enrichment_type: Literal["example", "mnemonic",
                             "visual", "related", "practical"]
    content: str = Field(min_length=1)
    rationale: str = Field(default="")


class ContextEnrichmentResult(BaseModel):
    """Result from context enrichment agent.

    Enhances cards with examples, mnemonics, and context.
    """

    model_config = ConfigDict(frozen=False)

    should_enrich: bool
    enriched_card: Optional[GeneratedCard] = None
    additions: list[EnrichmentAddition] = Field(default_factory=list)
    additions_summary: str = ""
    enrichment_rationale: str = ""
    enrichment_time: float = 0.0


class AgentPipelineResult(BaseModel):
    """Complete result from the multi-agent pipeline.

    Tracks results from pre-validation, generation, post-validation, and memorization quality stages.
    """

    model_config = ConfigDict(frozen=False)

    success: bool
    pre_validation: PreValidationResult
    generation: Optional[GenerationResult] = None
    post_validation: Optional[PostValidationResult] = None
    memorization_quality: Optional[MemorizationQualityResult] = None
    total_time: float = Field(ge=0.0)
    retry_count: int = Field(
        ge=0, description="Number of post-validation retries")
