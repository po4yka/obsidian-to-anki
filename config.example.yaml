# Obsidian to Anki Configuration
# Your custom setup for ~/Documents/InterviewQuestions

# Obsidian Configuration
vault_path: "~/Documents/InterviewQuestions"
source_dir: "." # Since your Q&A files are directly in subdirectories like 40-Android, 30-System-Design, etc.

# Anki Configuration
anki_connect_url: "http://127.0.0.1:8765"
anki_deck_name: "Interview Questions"
anki_note_type: "APF: Simple (3.0.0)"

# Runtime Settings
run_mode: "apply" # Options: apply, dry-run
delete_mode: "delete" # Options: delete, archive

# Database
db_path: ".sync_state.db"

# Logging
log_level: "INFO" # Options: DEBUG, INFO, WARN, ERROR

# =============================================================================
# AGENT SYSTEM CONFIGURATION
# =============================================================================

# Feature Flag: Use Multi-Agent AI System
# Enables the LangGraph multi-agent orchestration system
use_langgraph: true

# Agent Execution Mode
# - parallel: Faster but uses 30-34GB RAM (recommended for M4 Max)
# - sequential: Slower but lower memory usage
agent_execution_mode: "parallel"

# Task Queue Configuration (Redis-backed)
# Requires Redis server running
enable_queue: false
redis_url: "redis://localhost:6379"
queue_max_retries: 3


# Indexing LLM usage (disable to improve indexing speed and cost)
index_use_llm_extraction: false

# Enforce bilingual validation (require Q/A pairs for all languages)
# Default to False - validation done by LLM repair instead
enforce_bilingual_validation: false

# Card creation verification (verify cards exist in Anki after creation)
verify_card_creation: true

# Imperfect note processing settings
enable_content_generation: true # Allow LLM to generate missing content
repair_missing_sections: true # Generate missing language sections
tolerant_parsing: true # Allow notes with minor issues to proceed
parser_repair_generate_content: true # Enable content generation in repair agent

# Ollama Configuration
ollama_base_url: "http://localhost:11434"

# Parser-Repair Agent (Fixes malformed notes)
parser_repair_enabled: true
parser_repair_model: "qwen3:8b"
parser_repair_temperature: 0.0

# Pre-Validator Agent (Fast structural checks)
pre_validator_model: "qwen3:8b"
pre_validator_temperature: 0.0
pre_validation_enabled: true

# Generator Agent (Card generation)
generator_model: "qwen3:32b"
generator_temperature: 0.3

# Post-Validator Agent (Quality assurance)
post_validator_model: "qwen3:14b"
post_validator_temperature: 0.0
post_validation_max_retries: 3
post_validation_auto_fix: true
post_validation_strict_mode: true

# =============================================================================
# LLM PROVIDER SETTINGS (Unified system)
# =============================================================================

# Choose your LLM provider
# Options: "ollama", "openai", "anthropic", "claude", "openrouter", "lm_studio"
llm_provider: "ollama"

# Common LLM settings (applies to all providers)
llm_temperature: 0.2
llm_top_p: 0.3
llm_timeout: 600.0 # Increased for large models (qwen3:32b, qwen3:14b)
llm_max_tokens: 2048

# =============================================================================
# PROVIDER-SPECIFIC SETTINGS
# =============================================================================

# --- Ollama (Local/Cloud LLMs) ---
# ollama_base_url is already set above (line 37)
# ollama_api_key: ""  # Required only for Ollama Cloud

# --- OpenAI (GPT models) ---
# To use OpenAI: set llm_provider to "openai" and provide API key
# openai_api_key: "sk-..."  # Get from https://platform.openai.com/api-keys
# openai_base_url: "https://api.openai.com/v1"  # Custom endpoint (optional)
# openai_organization: ""  # Organization ID (optional)
# openai_max_retries: 3

# Recommended OpenAI models for each role:
# - generator_model: "gpt-4-turbo-preview" or "gpt-4"
# - post_validator_model: "gpt-4" or "gpt-3.5-turbo"
# - pre_validator_model: "gpt-3.5-turbo"

# --- Anthropic (Claude models) ---
# To use Claude: set llm_provider to "anthropic" or "claude" and provide API key
# anthropic_api_key: "sk-ant-..."  # Get from https://console.anthropic.com/
# anthropic_base_url: "https://api.anthropic.com"
# anthropic_api_version: "2023-06-01"
# anthropic_max_retries: 3

# Recommended Claude models for each role:
# - generator_model: "claude-3-opus-20240229" or "claude-3-sonnet-20240229"
# - post_validator_model: "claude-3-sonnet-20240229"
# - pre_validator_model: "claude-3-haiku-20240307"

# --- OpenRouter (Multi-model gateway) ---
# To use OpenRouter: set llm_provider to "openrouter" and provide API key
# openrouter_api_key: "sk-or-..."  # Get from https://openrouter.ai/keys
# openrouter_base_url: "https://openrouter.ai/api/v1"
# openrouter_site_url: "https://your-site.com"  # For ranking (optional)
# openrouter_site_name: "Your App Name"  # For ranking (optional)

# --- LM Studio (Local GUI-based LLMs) ---
# To use LM Studio: set llm_provider to "lm_studio" and ensure server is running
# lm_studio_base_url: "http://localhost:1234/v1"

# =============================================================================
# DECK EXPORT SETTINGS (Optional .apkg generation)
# =============================================================================

export_deck_name: "Interview Questions"
export_deck_description: "Generated from Obsidian notes"
export_output_path: "interview_questions.apkg"

# =============================================================================
# UNIFIED AGENT FRAMEWORK CONFIGURATION
# =============================================================================

# Primary agent framework selection
# Choose between 'pydantic_ai' (recommended, stable) or 'langchain' (new, advanced)
agent_framework: "pydantic_ai"

# LangChain Agent Type Configuration (only used when agent_framework = "langchain")
# Each agent type has different strengths:
# - tool_calling: Best for complex operations requiring multiple tools (recommended)
# - react: Best for reasoning and validation with transparent thought process
# - structured_chat: Best for multi-input scenarios
# - json_chat: Best for structured data processing
langchain_generator_type: "tool_calling" # Card generation with tool access
langchain_pre_validator_type: "react" # Pre-validation with reasoning
langchain_post_validator_type: "tool_calling" # Post-validation with tools
langchain_enrichment_type: "structured_chat" # Context enrichment with multiple inputs

# Agent Framework Fallback Configuration
# Automatic fallback when primary framework fails
agent_fallback_on_error: "pydantic_ai" # Fallback to PydanticAI on errors
agent_fallback_on_timeout: "react" # Fallback to ReAct on timeouts

# =============================================================================
# LEGACY LANGCHAIN AGENT SYSTEM (Deprecated - use unified config above)
# =============================================================================

# Feature Flag: Use legacy LangChain-based agent system
# This is the old agent system - consider migrating to the unified framework above
use_langchain_agents: false # Set to true to enable legacy system
