# Obsidian to Anki Configuration
# Your custom setup for ~/Documents/InterviewQuestions

# Obsidian Configuration
vault_path: "~/Documents/InterviewQuestions"
source_dir: "." # Since your Q&A files are directly in subdirectories like 40-Android, 30-System-Design, etc.

# Anki Configuration
anki_connect_url: "http://127.0.0.1:8765"
anki_deck_name: "Interview Questions"
anki_note_type: "APF: Simple (3.0.0)"

# Runtime Settings
run_mode: "apply" # Options: apply, dry-run
delete_mode: "delete" # Options: delete, archive

# Database
db_path: ".sync_state.db"

# Logging
log_level: "INFO" # Options: DEBUG, INFO, WARN, ERROR

# =============================================================================
# AGENT SYSTEM CONFIGURATION
# =============================================================================

# Feature Flag: Use Multi-Agent AI System
# Enables the LangGraph multi-agent orchestration system
use_langgraph: true

# Agent Execution Mode
# - parallel: Faster but uses 30-34GB RAM (recommended for M4 Max)
# - sequential: Slower but lower memory usage
agent_execution_mode: "parallel"

# Task Queue Configuration (Redis-backed)
# Requires Redis server running
enable_queue: false
redis_url: "redis://localhost:6379"
queue_max_retries: 3

# Indexing LLM usage (disable to improve indexing speed and cost)
index_use_llm_extraction: false

# Enforce bilingual validation (require Q/A pairs for all languages)
# Default to False - validation done by LLM repair instead
enforce_bilingual_validation: false

# Card creation verification (verify cards exist in Anki after creation)
verify_card_creation: true

# Imperfect note processing settings
enable_content_generation: true # Allow LLM to generate missing content
repair_missing_sections: true # Generate missing language sections
tolerant_parsing: true # Allow notes with minor issues to proceed
parser_repair_generate_content: true # Enable content generation in repair agent

# Ollama Configuration
ollama_base_url: "http://localhost:11434"

# Parser-Repair Agent (Fixes malformed notes)
parser_repair_enabled: true
pre_validation_enabled: true

# Post-Validator Agent (Quality assurance)
post_validation_max_retries: 3
post_validation_auto_fix: true
post_validation_strict_mode: true

# =============================================================================
# MODEL CONFIGURATION
# =============================================================================
# Model configuration uses preset system with optional overrides
# Preset options: "cost_effective", "balanced", "high_quality", "fast"
model_preset: "balanced"

# Default fallback model (used when preset doesn't cover a task)
default_llm_model: "qwen3:32b"

# Optional: Override specific tasks (only specify what differs from preset)
# Format: {'task_name': {'model_name': '...', 'temperature': 0.3, ...}}
# Task names: qa_extraction, parser_repair, pre_validation, generation,
#            post_validation, context_enrichment, memorization_quality, card_splitting,
#            duplicate_detection, reasoning, reflection, highlight
# model_overrides:
#   generation:
#     model_name: "qwen3:32b"
#     temperature: 0.3
#   pre_validation:
#     model_name: "qwen3:8b"
#     temperature: 0.0

# =============================================================================
# LLM PROVIDER SETTINGS (Unified system)
# =============================================================================

# Choose your LLM provider
# Options: "ollama", "openai", "anthropic", "claude", "openrouter", "lm_studio"
llm_provider: "ollama"

# Common LLM settings (applies to all providers)
llm_temperature: 0.2
llm_top_p: 0.3
llm_timeout: 600.0 # Increased for large models (qwen3:32b, qwen3:14b)
llm_max_tokens: 2048
llm_streaming_enabled: false # Enable SSE streaming for long generations
llm_reasoning_enabled: false
llm_reasoning_effort: "auto" # auto|minimal|low|medium|high|none
# reasoning_effort_overrides:
#     generation: "high"

# =============================================================================
# PROVIDER-SPECIFIC SETTINGS
# =============================================================================

# --- Ollama (Local/Cloud LLMs) ---
# ollama_base_url is already set above (line 37)
# ollama_api_key: ""  # Required only for Ollama Cloud

# --- OpenRouter (Multi-model gateway) ---
# Recommended models for each role (qwen/deepseek/kimi/minimax only):
# - generator_model: "qwen/qwen-2.5-72b-instruct" or "qwen/qwen3-next-80b-a3b-instruct"
# - post_validator_model: "deepseek/deepseek-chat" or "moonshotai/kimi-k2"
# - pre_validator_model: "qwen/qwen-2.5-32b-instruct"
# - context_enrichment_model: "minimax/minimax-m2"
# - card_splitting_model: "moonshotai/kimi-k2-thinking"

# --- OpenRouter (Multi-model gateway) ---
# To use OpenRouter: set llm_provider to "openrouter" and provide API key
# openrouter_api_key: "sk-or-..."  # Get from https://openrouter.ai/keys
# openrouter_base_url: "https://openrouter.ai/api/v1"
# openrouter_site_url: "https://your-site.com"  # For ranking (optional)
# openrouter_site_name: "Your App Name"  # For ranking (optional)

# --- LM Studio (Local GUI-based LLMs) ---
# To use LM Studio: set llm_provider to "lm_studio" and ensure server is running
# lm_studio_base_url: "http://localhost:1234/v1"

# =============================================================================
# DECK EXPORT SETTINGS (Optional .apkg generation)
# =============================================================================

export_deck_name: "Interview Questions"
export_deck_description: "Generated from Obsidian notes"
export_output_path: "interview_questions.apkg"
# =============================================================================
# AGENT FRAMEWORK CONFIGURATION
# =============================================================================

# Agent system uses LangGraph with PydanticAI for structured outputs

