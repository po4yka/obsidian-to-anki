# LangChain Agent System Architecture

## Overview

This document describes the architecture of the LangChain-based multi-agent system for converting Obsidian notes into validated Anki cards.

## System Goals

1. **Accurate Mapping**: Convert Obsidian note structure and front-matter to correct Anki note fields
2. **Strong Validation**: Enforce Anki model schema (fields, required properties, constraints)
3. **Semantic QA**: Ensure question and answer match and are pedagogically sound
4. **Safe Updates**: Support idempotent sync with existing cards
5. **Extensibility**: Allow for optional style refinement and bilingual handling

## Architecture Components

### 1. Data Flow

```
Obsidian Note (Markdown)
    ↓
Obsidian Parser (existing)
    ↓
NoteContext (JSON-serializable)
    ↓
LangChain Supervisor Orchestrator
    ↓
┌─────────────────────────────────────────┐
│  Multi-Agent Pipeline (LangChain Tools) │
│  1. Card Mapping Agent                  │
│  2. Schema Validation Tool              │
│  3. QA Agent                            │
│  4. Style/Hint Agent (optional)         │
│  5. Card Diff Agent (for updates)       │
└─────────────────────────────────────────┘
    ↓
CardDecision (create/update/skip/manual_review)
    ↓
Anki Sync Layer (existing)
    ↓
AnkiConnect API
```

### 2. Core Data Structures

#### NoteContext
- **Purpose**: Canonical representation of parsed Obsidian note
- **Source**: Generated by Obsidian parser
- **Fields**: slug, note_path, frontmatter, sections, existing_anki_note, config_profile
- **Format**: Pydantic model with JSON serialization

#### ProposedCard
- **Purpose**: Output of Card Mapping Agent
- **Contains**: card_type, model_name, deck_name, fields (Front/Back/Extra/Hint), tags
- **Metadata**: language, bilingual_mode, confidence, notes

#### QAReport
- **Purpose**: Semantic validation results
- **Contains**: qa_score (0-1), issues list, auto_fixed list
- **Issue Types**: answer_mismatch, front_leaks_answer, language_mismatch, style_issue

#### SchemaValidationResult
- **Purpose**: Anki model schema compliance check
- **Contains**: valid flag, errors list, warnings list
- **Non-LLM**: Pure Python validation for reliability

#### CardDiffResult
- **Purpose**: Comparison of existing vs proposed card
- **Contains**: changes list, should_update flag, risk_level
- **Use Case**: Safe updates without overwriting user edits

#### CardDecision
- **Purpose**: Final decision for sync layer
- **Contains**: action (create/update/skip/manual_review), proposed_card, qa_report, schema_validation, diff, messages
- **Consumer**: Anki Sync Layer

### 3. Agents and Tools

#### 3.1 Card Mapping Agent
- **Type**: LangChain Tool (LLM-powered)
- **Input**: NoteContext
- **Output**: ProposedCard
- **Responsibilities**:
  - Map note structure to Anki fields (Question → Front, Answer → Back)
  - Select appropriate card_type (Basic, Cloze, Custom)
  - Resolve deck_name and model_name from config
  - Handle bilingual content
  - Respect front-matter hints (card_type_hint, deck_hint)
- **Model**: Powerful model for nuanced mapping (e.g., qwen3:32b)

#### 3.2 Schema Validation Tool
- **Type**: Non-LLM Python Tool
- **Input**: ProposedCard
- **Output**: SchemaValidationResult
- **Checks**:
  - Model exists in Anki configuration
  - All required fields present and non-empty
  - No unknown fields
  - Cloze syntax validation for Cloze cards
  - Length constraints (configurable)
- **Implementation**: Direct Anki model introspection

#### 3.3 QA Agent
- **Type**: LangChain Tool (LLM-powered)
- **Input**: NoteContext + ProposedCard
- **Output**: QAReport
- **Responsibilities**:
  - Semantic correctness: Does Back answer Front?
  - Answer leakage: Does Front contain spoilers?
  - Language consistency: Matches declared language?
  - Completeness: Are critical details missing?
  - Pedagogical quality: Clear, standalone answer?
- **Technique**: Self-answering + semantic comparison
- **Model**: Fast, capable model (e.g., qwen3:14b)

#### 3.4 Style/Hint Agent
- **Type**: LangChain Tool (LLM-powered)
- **Input**: ProposedCard + NoteContext + QAReport
- **Output**: Refined ProposedCard
- **Responsibilities**:
  - Shorten overly long Front while preserving meaning
  - Ensure Back is clear and standalone
  - Generate concise Hint if missing
  - Enforce style consistency (interrogative vs statement)
- **Optional**: Can be disabled in config
- **Model**: Fast model (e.g., qwen3:8b)

#### 3.5 Card Diff Agent
- **Type**: LangChain Tool (LLM-assisted)
- **Input**: existing_anki_note + ProposedCard
- **Output**: CardDiffResult
- **Responsibilities**:
  - Field-by-field comparison
  - Classify change severity (cosmetic/content/structural)
  - Apply update policy (allow_content_updates config)
  - Calculate risk_level
- **Logic**: Hybrid (exact match + semantic similarity)

### 4. Supervisor Orchestrator

#### 4.1 Orchestration Logic

**Happy Path Flow:**
```python
1. Receive NoteContext
2. Call map_to_anki_fields() → ProposedCard
3. Call validate_anki_schema() → SchemaValidationResult
   - If invalid: retry with feedback (max_mapping_retries)
   - If still invalid: action = "manual_review"
4. [Optional] Call polish_card_style()
5. Call qa_check_card() → QAReport
   - If qa_score < min_qa_score or high severity issues:
     - If allow_auto_fix: retry mapping with QA feedback
     - Else: action = "manual_review"
6. If existing_anki_note present:
   - Call compare_cards() → CardDiffResult
   - Set action based on should_update and policy
7. Else:
   - action = "create" (if valid and QA passed)
8. Assemble CardDecision and return
```

**Retry Strategy:**
- Schema validation failures → re-map with error context
- QA failures → re-map with issue descriptions
- Maximum retries controlled by `max_mapping_retries` config
- Total LLM calls capped by `max_llm_calls_per_card`

**Error Handling:**
- All exceptions caught and logged
- Fallback: `action = "manual_review"` with error message
- No silent failures

#### 4.2 Configuration

```yaml
agents:
  enabled: true
  provider: "langchain"
  max_mapping_retries: 2
  min_qa_score: 0.8
  allow_auto_fix: true
  allow_content_updates: true
  max_llm_calls_per_card: 8

  model:
    name: "qwen-14b-instruct"
    temperature: 0.2
    max_tokens: 1024

  bilingual:
    mode: "front_back"  # or "separate_cards"

  style:
    enforce_short_front: true
    max_front_chars: 200
    max_back_chars: 1200
```

### 5. Integration with Existing System

#### 5.1 Entry Points

**CLI:**
```bash
obsidian-anki-sync sync --use-langchain-agents
obsidian-anki-sync sync --agents-config custom_agents.yaml
```

**Sync Engine Hook:**
```python
# In sync/engine.py:_generate_card()
if config.use_langchain_agents:
    # Convert existing data to NoteContext
    note_context = NoteContext.from_parsed_note(metadata, qa_pairs, ...)

    # Call LangChain orchestrator
    decision = langchain_orchestrator.process_note(note_context)

    # Convert CardDecision to existing Card format
    cards = decision_to_cards(decision)
else:
    # Use existing APF generator
    cards = apf_generator.generate_card(...)
```

#### 5.2 Adapter Layer

**Purpose**: Bridge between existing data models and new LangChain models

```python
class AgentSystemAdapter:
    @staticmethod
    def to_note_context(metadata, qa_pairs, config) → NoteContext

    @staticmethod
    def from_card_decision(decision) → list[Card]
```

### 6. Testing Strategy

#### 6.1 Unit Tests
- **Pydantic Models**: Serialization, validation, constraints
- **Schema Validator**: All validation rules, edge cases
- **Adapter Layer**: Conversion logic both directions

#### 6.2 Integration Tests
- **Agent Pipeline**: Mocked LLM responses, golden files
- **Orchestrator**: Retry logic, error handling, config variations
- **End-to-End**: Full NoteContext → CardDecision flow

#### 6.3 Snapshot Tests
- **Determinism**: Same input → same output (with fixed model/temp)
- **Regression**: Golden files for known-good cards

### 7. Observability

#### 7.1 Logging
- **Structured Logs**: JSON format with correlation IDs
- **Log Levels**:
  - INFO: Pipeline start/end, decisions
  - DEBUG: Intermediate outputs, LLM calls
  - WARNING: QA issues, retries
  - ERROR: Validation failures, exceptions

#### 7.2 Metrics (Future)
- Agent call counts and latency
- Retry rates per agent
- QA score distribution
- Action breakdown (create/update/skip/manual_review)

### 8. Future Extensions

#### 8.1 Multiple Cards per Note
- Extend Card Mapping Agent to generate list of cards
- Handle subquestions as separate cards

#### 8.2 Curriculum Agent
- Prioritize notes by graph centrality
- Topic coverage analysis

#### 8.3 Review Agent
- Periodic re-evaluation of existing cards
- Detect stale or outdated content

#### 8.4 Bilingual Separate Cards
- Generate two cards from one bilingual note
- Link cards with reverse relationship

## Design Decisions

### Why LangChain?
- **Tool Abstraction**: Clean interface for agents
- **Structured Output**: Pydantic integration for type safety
- **Ecosystem**: Easy to add tools, chains, retrievers
- **Debuggability**: LangSmith integration available

### Why Separate Agents?
- **Single Responsibility**: Each agent has clear purpose
- **Testability**: Mock/test agents independently
- **Configurability**: Enable/disable agents per use case
- **Maintainability**: Easier to debug and improve

### Why Schema Validation is Non-LLM?
- **Reliability**: No hallucination risk for rule-based checks
- **Performance**: Instant validation, no API calls
- **Determinism**: Same input always produces same result

### Why Supervisor Pattern?
- **Control Flow**: Centralized retry and error handling logic
- **Observability**: Single place to log pipeline execution
- **Flexibility**: Easy to reorder or skip agents

## Implementation Plan

1. **Phase 1**: Core Data Structures
   - Implement all Pydantic models
   - Write serialization/deserialization tests

2. **Phase 2**: Non-LLM Tools
   - Schema Validation Tool
   - Card Diff Agent (exact match logic)
   - Adapter layer

3. **Phase 3**: LLM Tools
   - LangChain provider setup
   - Card Mapping Agent
   - QA Agent
   - Style/Hint Agent

4. **Phase 4**: Orchestrator
   - Supervisor implementation
   - Retry and error handling logic
   - Configuration management

5. **Phase 5**: Integration
   - Sync engine hooks
   - CLI flags
   - Dry-run mode

6. **Phase 6**: Testing
   - Unit tests for all components
   - Integration tests with mocked LLMs
   - End-to-end tests

7. **Phase 7**: Documentation
   - User guide
   - Configuration reference
   - Troubleshooting guide

## File Structure

```
src/obsidian_anki_sync/
├── agents/
│   ├── langchain/                    # NEW
│   │   ├── __init__.py
│   │   ├── models.py                 # Pydantic data structures
│   │   ├── supervisor.py             # Supervisor Orchestrator
│   │   ├── tools/
│   │   │   ├── __init__.py
│   │   │   ├── card_mapper.py        # Card Mapping Agent
│   │   │   ├── schema_validator.py   # Schema Validation Tool
│   │   │   ├── qa_checker.py         # QA Agent
│   │   │   ├── style_polisher.py     # Style/Hint Agent
│   │   │   └── card_differ.py        # Card Diff Agent
│   │   ├── adapter.py                # Adapter to existing models
│   │   └── config.py                 # Agent config dataclass
│   ├── orchestrator.py               # EXISTING (keep for backward compat)
│   └── ...
├── providers/
│   ├── langchain_provider.py         # NEW: LangChain wrapper
│   └── ...
└── ...

tests/
├── agents/
│   ├── langchain/                    # NEW
│   │   ├── test_models.py
│   │   ├── test_supervisor.py
│   │   ├── test_card_mapper.py
│   │   ├── test_schema_validator.py
│   │   ├── test_qa_checker.py
│   │   ├── test_style_polisher.py
│   │   ├── test_card_differ.py
│   │   └── test_adapter.py
│   └── ...
└── ...
```

## Success Criteria

1. **Correctness**: 95%+ of generated cards pass schema validation
2. **Quality**: Average QA score > 0.85
3. **Safety**: No unintended overwrites of existing cards
4. **Performance**: < 10 seconds per card (with remote LLM)
5. **Reliability**: < 1% exception rate in production
6. **Maintainability**: 90%+ test coverage for agent system

---

**Document Version**: 1.0
**Last Updated**: 2025-11-10
**Authors**: Claude Code Agent System Team
